{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0bf5863",
   "metadata": {},
   "source": [
    "# Introduction to Logistic Regression\n",
    "\n",
    "## 1. What is Logistic Regression?\n",
    "\n",
    "- **Logistic Regression** is a **supervised learning algorithm** used for **classification** tasks.\n",
    "- Unlike linear regression, logistic regression is used when the dependent variable is **categorical** (binary or multiclass).\n",
    "- It models the probability of a particular class or event, using the **logistic function** (also known as the **sigmoid function**).\n",
    "- Output is a probability value between 0 and 1, which is then used to classify data into different classes (e.g., true/false, spam/not spam).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Logistic vs. Linear Regression\n",
    "\n",
    "- **Linear Regression**: Predicts a continuous output (regression tasks).\n",
    "- **Logistic Regression**: Predicts the probability of categorical outcomes (classification tasks).\n",
    "  \n",
    "The core idea is to transform the linear model output into a probability using the **sigmoid function**:\n",
    "  \n",
    "\\[\n",
    "\\hat{y} = \\frac{1}{1 + e^{-(wx + b)}}\n",
    "\\]\n",
    "\n",
    "- Here, `w` and `b` are weights and bias, `x` is the input, and `e` is Euler's number.\n",
    "- The output (`\\hat{y}`) is a probability between 0 and 1.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Sigmoid (Logistic) Function\n",
    "\n",
    "The logistic function converts the output of a linear equation into a probability:\n",
    "\n",
    "\\[\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "\\]\n",
    "\n",
    "- When `z` (the linear combination of inputs and weights) is large and positive, the output approaches 1.\n",
    "- When `z` is large and negative, the output approaches 0.\n",
    "- It maps any real-valued number into a value between 0 and 1.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Binary Classification\n",
    "\n",
    "- Logistic regression is typically used for **binary classification**, where the target variable has two classes, such as:\n",
    "  - **Spam or Not Spam**\n",
    "  - **Pass or Fail**\n",
    "  - **Disease or No Disease**\n",
    "\n",
    "For binary classification:\n",
    "- If the output probability is **> 0.5**, classify the observation as class **1**.\n",
    "- If the output probability is **<= 0.5**, classify the observation as class **0**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Decision Boundary\n",
    "\n",
    "- A **decision boundary** is the threshold at which logistic regression classifies an input into one class or another.\n",
    "- For binary classification, the decision boundary is often at **0.5**.\n",
    "- It separates the feature space into two regions: one for class 0 and one for class 1.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Cost Function in Logistic Regression\n",
    "\n",
    "Instead of the Mean Squared Error (MSE) used in linear regression, logistic regression uses a **log loss** (or **binary cross-entropy**) cost function:\n",
    "\n",
    "\\[\n",
    "\\text{Cost}(h(x), y) = - \\left[ y \\log(h(x)) + (1 - y) \\log(1 - h(x)) \\right]\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- `h(x)` is the predicted probability.\n",
    "- `y` is the actual class (0 or 1).\n",
    "\n",
    "This cost function penalizes incorrect predictions more heavily as the predicted probability diverges from the actual class label.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Multiclass Logistic Regression (One-vs-Rest)\n",
    "\n",
    "- **Binary logistic regression** is for two classes, but logistic regression can be extended to **multiclass classification** using the **One-vs-Rest (OvR)** approach.\n",
    "- In **One-vs-Rest**, the algorithm fits a binary classifier for each class against all other classes, then selects the class with the highest probability.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Example Code\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize and configure the logistic regression model\n",
    "model = LogisticRegression(random_state=0)\n",
    "\n",
    "# Fit the model to training data (X_train, y_train)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities on new data\n",
    "pred_probs = model.predict_proba(X_test)\n",
    "\n",
    "# Make class predictions (0 or 1)\n",
    "predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Evaluation Metrics for Logistic Regression\n",
    "\n",
    "- **Accuracy**: The ratio of correct predictions to total predictions.\n",
    "- **Precision**: The ratio of true positives to predicted positives.\n",
    "- **Recall**: The ratio of true positives to actual positives.\n",
    "- **F1 Score**: The harmonic mean of precision and recall, providing a single measure that balances both.\n",
    "\n",
    "\\[\n",
    "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "\\]\n",
    "\n",
    "- **ROC Curve (Receiver Operating Characteristic)**: Plots the true positive rate vs. the false positive rate at different threshold levels.\n",
    "- **AUC (Area Under Curve)**: Measures the overall performance of the model; a higher AUC indicates better performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Regularization in Logistic Regression\n",
    "\n",
    "- Logistic regression often uses **regularization** to prevent overfitting.\n",
    "- Common regularization types:\n",
    "  - **L2 Regularization (Ridge)**: Adds a penalty proportional to the square of the magnitude of the coefficients.\n",
    "  - **L1 Regularization (Lasso)**: Adds a penalty proportional to the absolute value of the coefficients, often resulting in sparse models (some weights become zero).\n",
    "\n",
    "The regularization parameter is controlled by the `C` parameter in scikit-learnâ€™s `LogisticRegression` model. A smaller `C` means stronger regularization.\n",
    "\n",
    "```python\n",
    "model = LogisticRegression(C=1.0, penalty='l2', solver='lbfgs')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Applications of Logistic Regression\n",
    "\n",
    "- **Medical Diagnostics**: Predict the likelihood of disease (e.g., diabetes, heart disease).\n",
    "- **Spam Detection**: Classify emails as spam or not spam.\n",
    "- **Credit Scoring**: Assess the risk of loan default.\n",
    "- **Marketing**: Predict whether a customer will respond to a campaign (response or no response).\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Limitations of Logistic Regression\n",
    "\n",
    "- Assumes a **linear relationship** between the features and the log-odds of the output.\n",
    "- Can underperform when the decision boundary is non-linear (other algorithms like decision trees or SVMs may be better).\n",
    "- Sensitive to **outliers**, as they can heavily influence the model.\n",
    "- Works best when there is little or no multicollinearity between features.\n",
    "\n",
    "---\n",
    "\n",
    "## 13. Summary\n",
    "\n",
    "- **Logistic Regression** is a simple yet powerful classification algorithm.\n",
    "- It uses the logistic function to model the probability of class membership.\n",
    "- Regularization can be applied to improve generalization.\n",
    "- It is widely used for binary classification tasks and can be extended to multiclass problems.\n",
    "\n",
    "--- \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
