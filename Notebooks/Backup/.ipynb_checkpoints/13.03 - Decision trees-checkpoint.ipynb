{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9291895f",
   "metadata": {},
   "source": [
    "# Introduction to Decision Trees\n",
    "\n",
    "## 1. What is a Decision Tree?\n",
    "\n",
    "- A **Decision Tree** is a **supervised learning algorithm** that can be used for both **classification** and **regression** tasks.\n",
    "- It models decisions as a tree-like structure, where:\n",
    "  - **Internal nodes** represent a feature or attribute on which the data is split.\n",
    "  - **Branches** represent the outcome of the decision based on that feature.\n",
    "  - **Leaf nodes** represent the final output or class label.\n",
    "- Decision trees are easy to interpret and can handle both numerical and categorical data.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. How Decision Trees Work\n",
    "\n",
    "### Steps:\n",
    "1. **Root Node**: Start with the entire dataset and choose the feature that best splits the data. This feature becomes the root of the tree.\n",
    "2. **Splitting**: At each internal node, the algorithm selects a feature and threshold to split the data in a way that maximizes the separation between different classes (for classification) or reduces the prediction error (for regression).\n",
    "3. **Recursion**: The splitting process continues recursively, creating branches and nodes, until one of the stopping conditions is met (e.g., maximum depth, minimum samples at a node).\n",
    "4. **Leaf Nodes**: The tree stops growing when a node cannot be split further, and the prediction (class or regression value) is made at the leaf nodes.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Decision Criteria\n",
    "\n",
    "To decide how to split the data at each node, decision trees use criteria like:\n",
    "\n",
    "- **Gini Impurity** (for classification): Measures the likelihood of incorrect classification at a node.\n",
    "  \n",
    "  \\[\n",
    "  Gini = 1 - \\sum (p_i)^2\n",
    "  \\]\n",
    "\n",
    "- **Entropy** (for classification): Measures the information gain from a split.\n",
    "  \n",
    "  \\[\n",
    "  Entropy = - \\sum p_i \\log_2(p_i)\n",
    "  \\]\n",
    "  \n",
    "- **Information Gain**: The decrease in entropy after a dataset is split on an attribute.\n",
    "  \n",
    "  \\[\n",
    "  \\text{Information Gain} = Entropy(parent) - \\sum \\left( \\frac{n_{child}}{n_{parent}} \\right) Entropy(child)\n",
    "  \\]\n",
    "\n",
    "- **Mean Squared Error (MSE)** (for regression): Measures the variance reduction at each split.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Tree Pruning\n",
    "\n",
    "- **Overfitting**: A decision tree can become overly complex and fit the noise in the training data. This leads to poor generalization to unseen data.\n",
    "- **Pruning** is a technique used to reduce the size of the tree and prevent overfitting.\n",
    "  - **Pre-pruning** (early stopping): Stop the tree from growing if certain conditions are met, such as reaching a maximum depth or minimum number of samples per node.\n",
    "  - **Post-pruning**: Build the entire tree and then remove branches that provide little benefit.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Important Parameters in Decision Trees\n",
    "\n",
    "- **max_depth**: The maximum depth of the tree. Limiting depth prevents overfitting.\n",
    "- **min_samples_split**: The minimum number of samples required to split an internal node.\n",
    "- **min_samples_leaf**: The minimum number of samples required to be in a leaf node.\n",
    "- **criterion**: The function used to measure the quality of a split (e.g., `gini` for Gini impurity or `entropy` for information gain in classification, `mse` for regression).\n",
    "- **max_features**: The number of features to consider when looking for the best split.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Example Code\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize and configure the decision tree model\n",
    "model = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=0)\n",
    "\n",
    "# Fit the model to training data (X_train, y_train)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on new data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Visualize the tree\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "tree.plot_tree(model, filled=True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Advantages of Decision Trees\n",
    "\n",
    "- **Interpretability**: Decision trees are easy to visualize and understand, even for non-technical stakeholders.\n",
    "- **No Need for Feature Scaling**: Unlike algorithms such as SVM or k-NN, decision trees do not require features to be scaled or normalized.\n",
    "- **Handles Both Numerical and Categorical Data**: It can work with different types of data without much preprocessing.\n",
    "- **Non-parametric**: Does not assume any specific distribution of data.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Disadvantages of Decision Trees\n",
    "\n",
    "- **Overfitting**: Decision trees can easily overfit the training data, especially if they are allowed to grow very deep. This can lead to poor generalization to new data.\n",
    "- **Unstable**: Small changes in the data can result in a completely different tree structure, which makes decision trees less robust.\n",
    "- **Biased Towards Features with More Categories**: Decision trees can favor attributes with many distinct values, which can lead to biased splits.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Evaluation Metrics for Decision Trees\n",
    "\n",
    "For classification tasks:\n",
    "- **Accuracy**: The ratio of correct predictions to total predictions.\n",
    "- **Precision, Recall, and F1 Score**: Useful for imbalanced datasets.\n",
    "  \n",
    "For regression tasks:\n",
    "- **Mean Squared Error (MSE)**: Measures the average squared difference between predicted and actual values.\n",
    "- **RÂ² (Coefficient of Determination)**: Indicates how well the predictions approximate the real data.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Regularization in Decision Trees\n",
    "\n",
    "To avoid overfitting, you can apply **regularization** techniques:\n",
    "- **max_depth**: Limits the depth of the tree, reducing overfitting.\n",
    "- **min_samples_split**: Sets the minimum number of samples required to split a node.\n",
    "- **min_samples_leaf**: Ensures that leaf nodes have a minimum number of samples.\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Decision Tree Variants\n",
    "\n",
    "- **Classification Trees (CART)**: Used for classification tasks where the target is categorical.\n",
    "- **Regression Trees**: Used for regression tasks where the target is continuous.\n",
    "- **Random Forests**: An ensemble of decision trees to improve accuracy and robustness.\n",
    "- **Gradient Boosting**: Combines multiple weak decision trees into a strong learner by optimizing residuals.\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Applications of Decision Trees\n",
    "\n",
    "- **Credit Scoring**: To predict whether a customer is likely to default on a loan.\n",
    "- **Medical Diagnosis**: To classify patients based on the presence or absence of a disease.\n",
    "- **Customer Segmentation**: To group customers based on purchasing behavior.\n",
    "- **Fraud Detection**: To detect anomalous behavior in financial transactions.\n",
    "\n",
    "---\n",
    "\n",
    "## 13. Limitations of Decision Trees\n",
    "\n",
    "- **Overfitting**: Decision trees are prone to overfitting if not pruned or regularized properly.\n",
    "- **Bias Toward Features with More Categories**: Decision trees can be biased toward features with many categories.\n",
    "- **Unstable**: A small change in the data can result in a significantly different tree.\n",
    "\n",
    "---\n",
    "\n",
    "## 14. Summary\n",
    "\n",
    "- **Decision Trees** are a powerful and interpretable model for classification and regression tasks.\n",
    "- They split data recursively based on feature values and output class labels (for classification) or continuous values (for regression).\n",
    "- Proper regularization (through depth control and pruning) is essential to avoid overfitting.\n",
    "- Decision trees are widely used across various domains due to their simplicity and interpretability.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
