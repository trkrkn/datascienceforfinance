{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f781b4c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Lecture 13 - Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d036ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 1. Overview "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8f3e00",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Machine learning** (ML) is a branch of artificial intelligence that involves **training algorithms** to make **predictions** or decisions based on data, **without being explicitly programmed**.\n",
    "\n",
    "Applications:\n",
    "\n",
    "- **Finance**: Stock price prediction, fraud detection, risk analysis, customer segmentation, etc.\n",
    "- **Other fields**: Image recognition, natural language processing, recommendation systems.\n",
    "    \n",
    "    \n",
    "In `Python`, `scikit-learn` is a powerful **library** for machine learning that provides tools for preprocessing data, training models, and evaluating them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e232ddf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "This notebook covers:\n",
    "- **Supervised** and **Unsupervised** learning\n",
    "    - **K-means clustering**\n",
    "    - **Decision trees**\n",
    "- **Data** preprocessing\n",
    "- Using `scikit-learn` :\n",
    "    - **Importing** ML model class\n",
    "    - **Instantiating** a model object\n",
    "    - **Fitting** the model object to data\n",
    "    - **Predicting** the outcome given the fitted model for some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93c3576-6bb4-4bce-8db3-b572e9bbfb28",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from pylab import mpl, plt\n",
    "\n",
    "plt.style.use('seaborn-v0_8-dark') \n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1000)\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8971e6c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 2. A primer on Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a9481f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Machine learning** is a **prediction technology**\n",
    "\n",
    "2 main steps: \n",
    "\n",
    "1. **Training/learning**: Learn patterns from previous outcomes (data)\n",
    "2. **Evaluation/deployment**: Given new inputs, predict most likely outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb306a34",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 2.1 Classes of machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ebd2a-b84a-4362-aaf7-b3cc9f945d64",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "There 3 broad classes of learning: **supervised**, **unsupervised**, **reinforcement**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f713d5-b9cd-46bf-9919-0a78efa79ce4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "1. **Supervised Learning**: \n",
    "\n",
    "The model learns from **labeled data** (i.e., input-output pairs). \n",
    "- Examples\n",
    "    - Regression: Predicting a continuous value (e.g., stock prices, real estate values).\n",
    "    - Classification: Predicting a categorical label (e.g., credit approval, fraud detection)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416c4147-b866-48d2-9612-07a49b5ff1ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "2. **Unsupervised Learning**: \n",
    "\n",
    "The model learns from **unlabeled data** to find hidden patterns or groupings. \n",
    "- Example\n",
    "    - Clustering: Grouping data points based on similarities (e.g., customer segmentation).\n",
    "    - Dimensionality reduction: Reducing the number of features while preserving the most important information (e.g., PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656d5d17-bfc9-4a18-b69e-d38c46ec9b29",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "3. **Reinforcement Learning** (skip)\n",
    "\n",
    "The model learns through interactions with an environment and feedback from its actions.\n",
    "- Example: Algorithmic trading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56058c6d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Visual intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed3e1d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Raw data**\n",
    "<center><img src=\"Figures/ml-learning-data.jpg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389d6982",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Supervised learning**\n",
    "\n",
    "Necessary dimension: **Labelled data**\n",
    "<center><img src=\"Figures/ml-learning-label.jpg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fa38f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Supervised learning**\n",
    "\n",
    "<center><img src=\"Figures/ml-learning-supervised.jpg\" width = 300></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48912666",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Unsupervised learning**\n",
    "\n",
    "<center><img src=\"Figures/ml-learning-unsupervised.jpg\" width = 300></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e392a79c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Other example**\n",
    "\n",
    "<center><img src=\"Figures/ml-learning-classes-1.jpg\" width = 700></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580f6fbe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 2.2 Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927f9702",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. Problem Definition\n",
    "2. Data\n",
    "    - Collection\n",
    "    - Preparation\n",
    "        - missing values, encoding of categorical variables, etc.\n",
    "    - Exploration\n",
    "    - Feature engineering\n",
    "        - create suitable features for the predictions\n",
    "    - **Split** (see below)\n",
    "        - training versus evaluation\n",
    "3. Learning\n",
    "    - Model selection\n",
    "    - Training\n",
    "    - Evaluation\n",
    "    - Tuning\n",
    "4. Deployment\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ac36af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 2.3 Sample split: training vs evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993f16ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A **sample split** in machine learning involves **dividing a dataset** into separate subsets to **train**, **validate**, and **test** a model. \n",
    "\n",
    "**Why split sata?**\n",
    "- **Evaluating Generalization**: To test how the model performs on unseen data.\n",
    "- **Preventing Overfitting**: Ensures the model isn't overly tailored to the training data.\n",
    "- **Model Tuning**: Provides a way to tune hyperparameters without bias from the training data.\n",
    "\n",
    "(Mostly for **supervised** learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7248cc3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The issue of overfitting**\n",
    "<center><img src = \"Figures/overfitting.png\"></img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e8ba47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835b1f57",
   "metadata": {},
   "source": [
    "1. **Training Set**: The model learns patterns and relationships from this data.\n",
    "   - **Purpose**: Used to train the machine learning model.\n",
    "   - **Size**: Typically 60-80% of the total data.\n",
    "2. **Validation Set**: Helps with model selection and prevents overfitting by providing feedback during training.\n",
    "   - **Purpose**: Used for tuning hyperparameters and evaluating the model during training.\n",
    "   - **Size**: Usually 10-20% of the total data.\n",
    "3. **Test Set**: Offers an unbiased assessment of model final performance on unseen data.\n",
    "   - **Purpose**: Used for final evaluation after training and validation.\n",
    "   - **Size**: Commonly 10-20% of the total data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eaa05f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Splitting Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba2d938",
   "metadata": {},
   "source": [
    "- **Random Split**: Randomly divides data into training, validation, and test sets. Common for general purposes.\n",
    "- **Stratified Split**: Ensures proportional representation of classes, useful for imbalanced datasets.\n",
    "- **Cross-Validation**: Splits data into `k` folds and trains the model `k` times, each time using a different fold as the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e31a059",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 3. Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e4a362",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In **unsupervised learning**, machine learning algorithms discover insights from raw data without any further guidance. \n",
    "\n",
    "One such algorithm is the **k-means clustering** algorithm which clusters a raw data set into a given number of subsets and assigns these subsets labels (`cluster 0`, `cluster 1`, etc.). \n",
    "\n",
    "In the following:\n",
    "- Generate clustered data (unlabeled)\n",
    "- Introduce K-Means\n",
    "- Apply the ML workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3edf30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe69de5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "`scikit-learn` allows the creation of sample data sets for different types of ML problems. \n",
    "\n",
    "The following uses the method `make_blobs()` to create a sample data set suited to illustrating k-means clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3810ec60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Data set creation with 4 clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed90820-24f0-40af-ac35-f1dfcc48f323",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=250, centers=4,\n",
    "                              random_state=500, cluster_std=1.25)\n",
    "\n",
    "plt.figure(figsize = (10,6));\n",
    "plt.scatter(X[:,0], X[:,1], s = 50);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c57019b",
   "metadata": {},
   "source": [
    "The goal of a clustering algorithm is therefore to recover the 4 clusters and properly classify each data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a388e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.2. K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069f140d-647a-4593-8014-6cf49a4fbe16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### A primer on K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20915a0e-d6d4-4c2b-b928-12ea7c0164c2",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**K-Means** is an **unsupervised learning algorithm** used for **clustering** data.\n",
    "\n",
    "- **Goal**: partition data into **`k` distinct clusters** based on similarities.\n",
    "    - Each data point is assigned to the **nearest cluster**\n",
    "        - Based on distance to the **centroid** $\\mu_k$ (the center of the cluster $k$).\n",
    "    - The algorithm re-assigns clusters in order to minimize the **inertia** $J$\n",
    "        - Sum of squared distances between points and their nearest cluster center.\n",
    "$$\n",
    "J = \\sum_{k=1}^{K} \\sum_{i \\in C_k} \\| x_i - \\mu_k \\|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c682dd62-0490-455b-9797-c445009dc9d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Algorithm**\n",
    "    1. **Initialization**: \n",
    "       - Choose `K` initial centroids randomly or using smarter methods (e.g., k-means++).\n",
    "    2. **Assignment**: \n",
    "       - Assign each data point to the nearest cluster centroid based on Euclidean distance.\n",
    "    3. **Update**:\n",
    "       - Recalculate the centroids by finding the mean of all points assigned to each cluster.\n",
    "    4. **Repeat**:\n",
    "       - Repeat the assignment and update steps until the centroids no longer change significantly (convergence).\n",
    "    5. **Output**:\n",
    "       - Final centroids and cluster assignments for each point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bbbe55-81cc-48a1-ac8f-588da4c74ea3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Visualisation**\n",
    "<center><img src=\"Figures/kmeans.png\" width = 800></img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f3f3c7-2e42-483a-8c16-c6c05584d30e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1e1e4a-d68b-40a3-9e5e-e0710c1ce849",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Step 1 - Importing and instantiating the `KMeans` model class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd1ccd3-d851-49df-ae1b-017823b8580b",
   "metadata": {},
   "source": [
    "The `scikit-learn` library contains a family class `cluster` with a subclass `KMeans`.\n",
    "\n",
    "```python\n",
    "    from sklearn.cluser import KMeans\n",
    "    model = KMeans (param)\n",
    "```\n",
    "Parameters for the instantiation of a `KMeans` object:\n",
    "- `n_clusters`: Number of clusters to form.\n",
    "- `random_state`: Ensures reproducibility by fixing the random initialization of centroids.\n",
    "- `max_iter`: The maximum number of iterations to run the algorithm.\n",
    "- `init`: The method for initializing centroids (e.g., 'random', 'k-means++').\n",
    "- `tol`: The tolerance to declare convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1976060-34a9-4eb3-856b-9e163a3a1839",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Step 2 - Fitting the model to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbe50f1-3aaa-4787-9a04-3f7616d822b9",
   "metadata": {},
   "source": [
    "Fitting the model to the data obtains with the method `.fit()` with the data as input.\n",
    "\n",
    "```python\n",
    "    model.fit(X)\n",
    "```\n",
    "\n",
    "This is where the optimization takes place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15f1829-b16f-437e-8612-7366d839aa38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Step 3 - Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eda1a09-1f3d-4422-bf0e-e3b500a6dc87",
   "metadata": {},
   "source": [
    "Predicting clusters obtains with the method `.predict()` with the data as input.\n",
    "\n",
    "```python\n",
    "    labels = model.predict(X)\n",
    "```\n",
    "\n",
    "The method associates a cluster label to each data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d665a-8c8c-4bb4-8357-68a468a09386",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca91c4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "model = KMeans (n_clusters = 4, random_state = 0)\n",
    "model.fit(X)\n",
    "y_kmeans = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6565e316",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "# Plot true values (y)\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm')\n",
    "axes[0].set_title(\"True Labels (y)\")\n",
    "# Plot predicted values (y_kmeans)\n",
    "axes[1].scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='coolwarm')\n",
    "axes[1].set_title(\"Predicted Labels (y_kmeans)\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91830f7f-8c9b-49ec-af27-c2c84baaf103",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Step 4 - Parameter tuning (`k`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b83236-4457-4e32-9628-4fbc12da4840",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Selecting the **optimal number** of `k` clusters is crucial for meaningful results.\n",
    "\n",
    "- Common techniques to determine `K`:\n",
    "  - **Elbow Method**: Plot the inertia for different values of `K` and look for an \"elbow\" where the decrease in inertia slows down.\n",
    "  - **Silhouette Score**: Measures how similar a point is to its cluster compared to other clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23781f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 4. Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa03e02b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In **supervised learning**, machine learning is achieved with **guidance** in the form of known results or **labelled** data. \n",
    "\n",
    "- With unsupervised learning, algorithms originate their own categorical labels of clusters identified.\n",
    "\n",
    "- In supervised learning, labels are given. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7274cb-8ec8-419a-822e-1f4252576e50",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Types of supervised learning algorithms**\n",
    "\n",
    "| Algorithm                | Quick Description                          | Common Use Cases                      |\n",
    "|--------------------------|--------------------------------------------|----------------------------------------|\n",
    "| Linear Regression        | A model that fits a linear relationship    | Predicting continuous values          |\n",
    "| Logistic Regression      | A model for binary/multiclass classification | Binary/multiclass classification    |\n",
    "| Decision Trees           | Tree-like structure for decision making    | Easy-to-interpret models              |\n",
    "| Support Vector Machines  | Finds the optimal boundary for classification | High-dimensional data              |\n",
    "| Random Forest            | Ensemble of decision trees for better performance | Ensemble learning for robust predictions |\n",
    "| Gradient Boosting        | Combines weak models to form a strong model | Improving weak learners               |\n",
    "| Neural Networks          | Mimics the human brain with layers of nodes | Complex patterns and non-linear relationships |\n",
    "| Ridge/Lasso Regression   | Linear models with regularization to prevent overfitting | Regularized linear models            |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d213f33-c506-4625-8d15-0f78cf782398",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the following:\n",
    "- Generate labelled data\n",
    "- Introduce Logistic regressions\n",
    "    - Apply the ML workflow\n",
    "        - Introduce performance evaluation\n",
    "- Introduce Decision trees\n",
    "    - Apply the ML workflow\n",
    "        - With sample split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14342ab8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.1 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d0b34d-3e63-407b-8a83-3c476088af49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`scikit-learn` allows the creation of sample data sets for supervised ML problems.\n",
    "\n",
    "The following uses the method `make_classification()` to create a sample data set suited to illustrating classification techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bf565b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Data set creation with 2 classes and 20 features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa256a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "n_samples = 10000\n",
    "X, y = make_classification(n_samples=n_samples, n_features=20,\n",
    "                                         n_informative=20, n_redundant=0,\n",
    "                                         n_repeated=0, random_state=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f71bed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap='coolwarm', marker='o');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe511e4",
   "metadata": {},
   "source": [
    "The goal of a classification algorithm is therefore to classify each 20-dimensional data point according to the binary value `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ca498b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.2 Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f81fa4-c91d-4d79-b39e-235fb851f28d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### A primer on Logistic regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc66f80",
   "metadata": {},
   "source": [
    "**Logistic regressions** model the probability of a particular class, using the **logistic function** (also known as the **sigmoid function**).\n",
    "- **Linear Regression**: Predicts a continuous output (regression tasks).\n",
    "- **Logistic Regression**: Predicts the probability of categorical outcomes (classification tasks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a5498b-d1f0-4e32-b1dd-7c96e57daf42",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The output is a probability value between 0 and 1, which is then used to classify data into different classes (e.g., true/false)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179b7980-cfe7-4edc-b63f-737ad5833b70",
   "metadata": {},
   "source": [
    "<center><img src=\"Figures/logistic.png\" width = 800></img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159df832-a9cb-40d9-8661-14f1c2f66928",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29913669-a45b-4fae-8c6b-df5e74b84b1c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Step 1 - Importing and instantiating the `Logistic` model class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e8682f-9ebe-49a2-9414-303922a30a29",
   "metadata": {},
   "source": [
    "The `scikit-learn` library contains a family class `linear_model` with a subclass `LogisticRegression`.\n",
    "\n",
    "```python\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    model = LogisticRegression(param)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189a4bc3-6ce1-4503-bd3d-4ae724040711",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Step 2 - Fitting the model to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe34e72-ab93-4acd-9acc-d4ce70d5b523",
   "metadata": {},
   "source": [
    "Fitting the model to the data obtains with the method `.fit()` with the data **and label** as input.\n",
    "\n",
    "```python\n",
    "    model.fit(X,y)\n",
    "```\n",
    "\n",
    "This is where the optimization takes place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7dc72f-1b1b-4568-b0be-5d136600c9ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Step 3 - Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ede0e8-7709-4906-8976-b92fd96656cf",
   "metadata": {},
   "source": [
    "Predicting labels obtains with the method `.predict` with the data as input.\n",
    "\n",
    "```python\n",
    "    pred = model.predict(X)\n",
    "```\n",
    "\n",
    "The method associates a prediction output to each data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fa21c3-192b-4841-a101-8a4adb8732d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f170bca1-90db-4d54-84ee-f1204750dcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# model = LogisticRegression(C = 1, solver = 'lbfgs')\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X,y)\n",
    "\n",
    "pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106c74c6-9b48-4f68-83ea-c2694fb2aa89",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Step 4 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb0e883",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xc = X[y == pred]\n",
    "Xf = X[y != pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3160b5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x=Xc[:, 0], y=Xc[:, 1], c=y[y == pred],\n",
    "                         marker='o', cmap='coolwarm')\n",
    "plt.scatter(x=Xf[:, 0], y=Xf[:, 1], c=y[y != pred],\n",
    "                         marker='x', cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0270e612-f267-40ad-b961-be46b39f28f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Given the prediction of the model, there are several ways to **assess the performance** of the model. \n",
    "\n",
    "**Confusion matrix**\n",
    "\n",
    "| Actual \\ Predicted | Positive Prediction | Negative Prediction |\n",
    "|---------------------|----------------------------|------------------------------|\n",
    "| Positive   | True Positive (TP)         | False Negative (FN)          |\n",
    "| Negative   | False Positive (FP)        | True Negative (TN)           |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02dd925-ac16-404e-8c8b-20eb511c16ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Performance metrics**\n",
    "- **Accuracy**: The ratio of correct predictions to total predictions.\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "- **Precision**: The ratio of true positives to predicted positives.\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "- **Recall**: The ratio of true positives to actual positives.\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "\n",
    "- **F1 Score**: The harmonic mean of precision and recall, providing a single measure that balances both.\n",
    "\n",
    "$$\n",
    "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732c267d-9c58-4ec0-a660-4cfe007e1ce7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y, pred)\n",
    "precision = precision_score(y, pred)  \n",
    "recall = recall_score(y, pred)  \n",
    "f1 = f1_score(y, pred)              \n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b894dcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.3 Decision trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f82639-8926-443b-a67d-c847d0740069",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### A primer on Decision tress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192f7827-0681-4423-bc25-c431e81a9f55",
   "metadata": {},
   "source": [
    "**Decision trees** model decisions as a tree-like structure, where:\n",
    "  - **Internal nodes** represent a feature or attribute on which the data is split.\n",
    "  - **Branches** represent the outcome of the decision based on that feature.\n",
    "  - **Leaf nodes** represent the final output or class label.\n",
    "\n",
    "Decision trees are easy to interpret and can handle both numerical and categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4e174a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example of a <u>trained</u> decision tree** (depth = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ff0e45",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "                           (Root) Credit Score >= 650?\n",
    "                          /                            \\\n",
    "                    Yes                                  No\n",
    "              Income >= 3000?                           Deny\n",
    "             /               \\\n",
    "          Yes                No\n",
    "       Loan Amount <= 20000?    Deny\n",
    "        /         \\\n",
    "      Yes         No\n",
    "    Approve      Deny"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577e986b-8412-47b1-ae8e-80f2800e4287",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The issue of overfitting**\n",
    "\n",
    "With enough **depth**, a decision tree can fit any data. Need to **split** data before engaging in training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f502902-fb9e-42ee-aa85-5c328c4434fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8939dbee-caa0-4360-8116-2e58ce9870ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Step 1 - Train-test split data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af238f97-c71f-4d69-8702-56dc164100b1",
   "metadata": {},
   "source": [
    "**Recall**: data needs to be split in 2 parts:\n",
    "  1. **Training Set**: Used to train the model (70-80% of the data)\n",
    "  3. **Test Set**: Used to evaluate the model's performance (20-30% of the data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b729b3f-bf74-460e-8192-2b465554703e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The `scikit-learn` library contains a family class `model_selection` with a subclass `train_test_split`.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3279c4ad-52f0-48a0-8e41-229985ee209a",
   "metadata": {},
   "source": [
    "Parameters in `train_test_split`\n",
    "\n",
    "- **`test_size`**: Specifies the proportion of the dataset to include in the test split. For example, `test_size=0.2` means 20% of the data will be used as the test set.\n",
    "- **`random_state`**: Controls the shuffling of data before splitting. A fixed `random_state` ensures reproducibility, meaning the same split will be produced each time.\n",
    "- **`shuffle`**: By default, the data is shuffled before splitting to ensure a random distribution between training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a4fc14-9d84-4239-a0ec-4f4061c4de57",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Step 2 - Importing and instantiating the `DecisionTree` model class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505e757f-cc13-47d0-92ec-f0e4f32ef8c4",
   "metadata": {},
   "source": [
    "The `scikit-learn` library contains a family class `tree` with a subclass `DecisionTreeClassifier`.\n",
    "\n",
    "```python\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    model = DecisionTreeClassifier(param)\n",
    "```\n",
    "With parameters:\n",
    "- `max_depth`: The maximum depth of the tree. Limiting depth prevents overfitting.\n",
    "- `min_samples_split`: The minimum number of samples required to split an internal node.\n",
    "- `min_samples_leaf`: The minimum number of samples required to be in a leaf node.\n",
    "- `criterion`: The function used to measure the quality of a split (e.g., `gini` for Gini impurity or `entropy` for information gain in classification, `mse` for regression).\n",
    "- `max_features`: The number of features to consider when looking for the best split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8217b725-9c11-4860-b2f0-ca4bcfc901f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Step 3 - Fitting the model to data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b196011b-0d45-4e50-9591-42b5d23e4b77",
   "metadata": {},
   "source": [
    "Fitting the model to the data obtains with the method `.fit()` with **the training data and label as input**.\n",
    "```python\n",
    "    model.fit(X_train,y_train)\n",
    "```\n",
    "This is where the optimization takes place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1883183c-702b-44d1-8da3-38993fe6e12f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Step 4 - Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa61545c-e5c2-46c0-8f03-1a2d51593117",
   "metadata": {},
   "source": [
    "Predicting clusters obtains with the method `.predict()` with the **test data** as input.\n",
    "```python\n",
    "    y_pred = model.predict_proba(X_test)\n",
    "```\n",
    "The method associates an output prediction to each data point in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228126ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d76ba6d-8b1a-4547-9d06-6927c8d1cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Output the sizes of the training and test sets\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62712a0e-6bb5-4e34-86e4-d8a7748ddd42",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Starting with `depth = 1`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d42f520-071e-4753-b41b-0a8ad8eff5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8e944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbae1c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e61ee17-292e-4bd7-8212-ee96e19609cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Step 5 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46846047-2cdd-47dd-bd62-32c8fc3edaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "precision = precision_score(y_test,y_pred)\n",
    "recall = recall_score(y_test,y_pred)\n",
    "f1 = f1_score(y_test,y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b52a9b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Xc = X_test[y_test == y_pred]\n",
    "Xf = X_test[y_test != y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e7d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x=Xc[:, 0], y=Xc[:, 1], c=y_test[y_test == y_pred],\n",
    "                         marker='o', cmap='coolwarm')\n",
    "plt.scatter(x=Xf[:, 0], y=Xf[:, 1], c=y_test[y_test != y_pred],\n",
    "                         marker='x', cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac28ed34",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Increasing maximum depth**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b9d344",
   "metadata": {},
   "source": [
    "*with training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c823d03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:>8s} | {:10s} | {:10s} | {:10s} | {:10s}'.format('depth', 'accuracy', 'precision', 'recall', 'f1')) \n",
    "print(60 * '-')\n",
    "for depth in range(1, 20):\n",
    "    model = DecisionTreeClassifier(max_depth=depth) \n",
    "    model.fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_test, model.predict(X_test)) \n",
    "    # accuracy = accuracy_score(y_test,y_pred)\n",
    "    prec = precision_score(y_test,model.predict(X_test))\n",
    "    rec = recall_score(y_test,model.predict(X_test))\n",
    "    f1 = f1_score(y_test,model.predict(X_test))\n",
    "    print('{:8d} | {:10.2f} | {:10.2f} | {:10.2f} | {:10.2f}'.format(depth, acc, prec, rec, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caed16fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*with overfitting*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42565810-1530-4574-972e-00f8c95a4742",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('{:>8s} | {:10s} | {:10s} | {:10s} | {:10s}'.format('depth', 'accuracy', 'precision', 'recall', 'f1')) \n",
    "print(60 * '-')\n",
    "for depth in range(1, 20):\n",
    "    model = DecisionTreeClassifier(max_depth=depth) \n",
    "    model.fit(X, y)\n",
    "    acc = accuracy_score(y, model.predict(X)) \n",
    "    # accuracy = accuracy_score(y_test,y_pred)\n",
    "    prec = precision_score(y,model.predict(X))\n",
    "    rec = recall_score(y,model.predict(X))\n",
    "    f1 = f1_score(y,model.predict(X))\n",
    "    print('{:8d} | {:10.2f} | {:10.2f} | {:10.2f} | {:10.2f}'.format(depth, acc, prec, rec, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b1392b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some applications in Finance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b132a1d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Credit card fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88a761c",
   "metadata": {},
   "source": [
    ">It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3d7d59",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this example, we’ll use a **decision tree classifier** to detect fraudulent credit card transactions. \n",
    "\n",
    "The target variable is `Class`, where `0` indicates non-fraudulent transactions, and `1` indicates fraudulent transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a55022",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099a4edb",
   "metadata": {},
   "source": [
    "- The dataset contains transactions made by credit cards in September 2013 by European cardholders. \n",
    "- This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. \n",
    "- The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "- Due to confidentiality issues, it contains only numerical input variables which are the result of a PCA transformation. \n",
    "- The only features which have not been transformed with PCA are 'Time' and 'Amount'. \n",
    "    - Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. \n",
    "    - The feature 'Amount' is the transaction Amount.\n",
    "- Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3ed0fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73ae6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5803ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Step 2: Import and prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a0a6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/13/creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38646c53",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40920a89",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6028a3e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Decompose input and label data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff86002",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Class'])\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a54cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Step 3: Split the Data\n",
    "\n",
    "To evaluate the model, split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c761344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618db88b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Step 4: Train the Decision Tree Model\n",
    "\n",
    "Initialize and train the decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d7986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the decision tree classifier\n",
    "clf = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a59046",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Step 5: Make Predictions\n",
    "\n",
    "Use the trained model to predict fraud on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2ea329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a348e50",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Step 6: Evaluate the Model\n",
    "\n",
    "Assess the model’s performance using accuracy, confusion matrix, and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247ede06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418f86dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Step 7: Visualize the Decision Tree\n",
    "\n",
    "Plotting the tree helps in understanding the decisions being made by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e165bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "plot_tree(clf, feature_names=X.columns, class_names=[\"Non-Fraud\", \"Fraud\"], filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd641688",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942cd930",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<u> Exercise </u>: Code interpreation with the following information\n",
    "\n",
    "- Data set: `Binance_Data.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0965e5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Data/13/Binance_Data.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaf2ebb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Machine learning method: K-Means and Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3d4b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faec106",
   "metadata": {},
   "source": [
    "> `StandardScaler` is a preprocessing tool from the `sklearn.preprocessing` module in the `Scikit-Learn` library. It’s used to standardize or normalize data, which means scaling the features so that they have a mean of zero and a standard deviation of one. This is commonly done to improve the performance of machine learning models, especially those sensitive to the scale of input data, like K-Means clustering, logistic regression, and neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110f3652",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053375ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7621201c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Interpret the following code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c190cc44",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df['Close_Price_Change'] = df['close'].pct_change().fillna(0) * 100\n",
    "df['Volatility'] = df['high'] - df['low']  \n",
    "\n",
    "data_for_clustering = df[['volume', 'Close_Price_Change', 'Volatility']].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_for_clustering)\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=0)\n",
    "data_for_clustering['Cluster'] = kmeans.fit_predict(data_scaled)\n",
    "\n",
    "data_for_clustering['Distance_to_Center'] = np.linalg.norm(data_scaled - kmeans.cluster_centers_[data_for_clustering['Cluster']], axis=1)\n",
    "\n",
    "threshold = data_for_clustering['Distance_to_Center'].quantile(0.95)\n",
    "data_for_clustering['Anomaly'] = data_for_clustering['Distance_to_Center'] > threshold\n",
    "\n",
    "data_for_clustering.head(10)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
