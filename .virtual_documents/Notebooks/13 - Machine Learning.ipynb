
















































































import numpy as np 
import pandas as pd
import datetime as dt
from pylab import mpl, plt


plt.style.use('seaborn')
mpl.rcParams['font.family'] = 'serif'
%matplotlib inline


np.random.seed(1000)
np.set_printoptions(suppress=True, precision=4)


from sklearn.datasets import make_blobs


X, y = make_blobs(n_samples=250, centers=4,
                              random_state=500, cluster_std=1.25)


plt.figure(figsize = (10,6));
plt.scatter(X[:,0], X[:,1], s = 50);








from sklearn.cluster import KMeans


model = KMeans (n_clusters = 4, random_state = 0)


model.fit(X)


y_kmeans = model.predict(X)


y_kmeans[:12]


plt.figure(figsize=(10, 6))
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='coolwarm');











from sklearn.datasets import make_classification


n_samples = 100
X, y = make_classification(n_samples=n_samples, n_features=2,
                                         n_informative=2, n_redundant=0,
                                         n_repeated=0, random_state=250)


X[:5]


X.shape


y[:5]


y.shape


plt.figure(figsize = (10,6))
plt.scatter(x=X[:, 0], y=X[:, 1], c=y, cmap='coolwarm')








from sklearn.linear_model import LogisticRegression


model = LogisticRegression(C = 1, solver = 'lbfgs')


model.fit(X,y)


model.predict_proba(X).round(4)[:5]


pred = model.predict(X)


from sklearn.metrics import accuracy_score


accuracy_score (y,pred)


Xc = X[y == pred]
Xf = X[y != pred]


plt.figure(figsize=(10, 6))
plt.scatter(x=Xc[:, 0], y=Xc[:, 1], c=y[y == pred],
                         marker='o', cmap='coolwarm')
plt.scatter(x=Xf[:, 0], y=Xf[:, 1], c=y[y != pred],
                         marker='x', cmap='coolwarm');








from sklearn.tree import DecisionTreeClassifier


model = DecisionTreeClassifier(max_depth=1)


model.fit(X,y)


model.predict_proba(X).round(4)[:5]


pred = model.predict(X)


accuracy_score(y,pred)


Xc = X[y == pred]
Xf = X[y != pred]


plt.figure(figsize=(10, 6))
plt.scatter(x=Xc[:, 0], y=Xc[:, 1], c=y[y == pred],
                         marker='o', cmap='coolwarm')
plt.scatter(x=Xf[:, 0], y=Xf[:, 1], c=y[y != pred],
                         marker='x', cmap='coolwarm');





print('{:>8s} | {:8s}'.format('depth', 'accuracy')) 
print(20 * '-')
for depth in range(1, 7):
    model = DecisionTreeClassifier(max_depth=depth) 
    model.fit(X, y)
    acc = accuracy_score(y, model.predict(X)) 
    print('{:8d} | {:8.2f}'.format(depth, acc))


















































from sklearn.datasets import load_iris

# Load the Iris dataset
iris = load_iris()
X = iris.data  # Features
y = iris.target  # Target variable (class labels)








from sklearn.model_selection import train_test_split

# Split the data into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Output the sizes of the training and test sets
print(f"Training set size: {X_train.shape}")
print(f"Test set size: {X_test.shape}")








from sklearn.ensemble import RandomForestClassifier

# Initialize the Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model on the training data
clf.fit(X_train, y_train)








from sklearn.metrics import accuracy_score

# Predict class labels on the test set
y_pred = clf.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy on the test set: {accuracy:.2f}")








import matplotlib.pyplot as plt
import numpy as np

# Get the feature importances
feature_importances = clf.feature_importances_

# Plot the feature importances
plt.figure(figsize=(8, 6))
plt.barh(iris.feature_names, feature_importances)
plt.xlabel("Feature Importance")
plt.ylabel("Feature")
plt.title("Feature Importance in Random Forest Classifier")
plt.show()








# 1. Load Iris Dataset
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# Load dataset
iris = load_iris()
X = iris.data
y = iris.target

# 2. Train-Test Split (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. Train Random Forest Classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

# 4. Predict on Test Set
y_pred = clf.predict(X_test)

# 5. Evaluate Model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy on the test set: {accuracy:.2f}")

# 6. Visualize Feature Importance
feature_importances = clf.feature_importances_
plt.figure(figsize=(8, 6))
plt.barh(iris.feature_names, feature_importances)
plt.xlabel("Feature Importance")
plt.ylabel("Feature")
plt.title("Feature Importance in Random Forest Classifier")
plt.show()



